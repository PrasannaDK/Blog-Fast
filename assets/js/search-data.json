{
  
    
        "post0": {
            "title": "EC - COVID-19 India Matplotlib Overview (Updated Daily)",
            "content": "An Exception was encountered at &#39;In [7]&#39;. . Execution using papermill encountered an exception here and stopped: . (Group, 2020) . Group, C. O. V. I. D.-19 I. O. D. O. (2020). COVID-19 INDIA TRACKER. https://api.covid19india.org/ | .",
            "url": "https://prasannadk.github.io/UnivAI-FastBlogs/fastpages/jupyter/2020/08/08/COVID-19-Dashboard.html",
            "relUrl": "/fastpages/jupyter/2020/08/08/COVID-19-Dashboard.html",
            "date": " • Aug 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "DYOD - Using ANN from Kudzu Lirary",
            "content": "%matplotlib inline import numpy as np import matplotlib.pyplot as plt import pandas as pd . Binary Classification between 3 and 8 . The scope and objective of the following code is only to distinguish or classify images as either a 3 or 8 . Getting the Data . import mnist . #collapse_show train_images = mnist.train_images() train_labels = mnist.train_labels() test_images = mnist.test_images() test_labels = mnist.test_labels() . . Displaying a random image . 3 . Preparing Data: Filter data to get 3 and 8 out . #collapse_show train_filter = np.where((train_labels == 3 ) | (train_labels == 8)) test_filter = np.where((test_labels == 3) | (test_labels == 8)) X_train, y_train = train_images[train_filter], train_labels[train_filter] X_test, y_test = test_images[test_filter], test_labels[test_filter] . . 8 . We normalize the pizel values in the 0 to 1 range . And setup the labels as 1 (when the digit is 3) and 0 (when the digit is 8) . We reshape the data to flatten the image pixels into a set of features or co-variates . Multi-Layer Perceptron Model . Setup . The following configuration of layers was used to set up the model for training: . A first affine layer which has 784 inputs and does 100 affine transforms. These are followed by a Relu | A second affine layer which has 100 inputs from the 100 activations of the past layer, and does 100 affine transforms. These are followed by a Relu | A third affine layer which has 100 activations and does 2 affine transformations to create an embedding for visualization. There is no non-linearity here. | A final &quot;logistic regression&quot; which has an affine transform from 2 inputs to 1 output, which is squeezed through a sigmoid. | . from kudzu.data import Data, Dataloader, Sampler from kudzu.callbacks import AccCallback, ClfCallback from kudzu.loss import MSE, BCE from kudzu.layer import Affine, Sigmoid, Relu, Tanh from kudzu.model import Model from kudzu.optim import GD from kudzu.train import Learner from collections import namedtuple . #collapse_show #Setting up classes data = Data(dataSet.X_train, dataSet.y_train) loss = BCE() opt = GD(config.lr) sampler = Sampler(data, config.bs, shuffle=True) dl = Dataloader(data, sampler) . . #collapse_show #Setting up model nn_layers = [Affine(&quot;FirstAffine&quot;, 784, 100), Relu(&quot;FirstRelu&quot;), Affine(&quot;SecondAffine&quot;, 100, 100), Relu(&quot;SecondRelu&quot;), Affine(&quot;Embedding&quot;, 100, 2), Affine(&quot;LastAffine&quot;, 2, 1), Sigmoid(&quot;LastSigmoid&quot;)] nn_model = Model(nn_layers) . . #collapse_show #Setting up training nn_learner = Learner(loss, nn_model, opt, config.num_epochs) nn_acc = ClfCallback(nn_learner, config.bs, dataSet) nn_learner.set_callbacks([nn_acc]) . . Traning the Multi Layer Perceptron . nn_learner.train_loop(dl) . Epoch 0 Loss 0.6125921643011659 Epoch 0 Train-Accuracy 0.8586212652311801 Test-Accuracy 0.8855846774193549 Epoch 10 Loss 0.1370228401740017 Epoch 10 Train-Accuracy 0.957352695710232 Test-Accuracy 0.9647177419354839 Epoch 20 Loss 0.10535273583368696 Epoch 20 Train-Accuracy 0.9680353864129527 Test-Accuracy 0.9727822580645161 Epoch 30 Loss 0.09172208796439074 Epoch 30 Train-Accuracy 0.972041395426473 Test-Accuracy 0.9753024193548387 Epoch 40 Loss 0.08239759690118895 Epoch 40 Train-Accuracy 0.9752128192288433 Test-Accuracy 0.9753024193548387 Epoch 50 Loss 0.07485683838533544 Epoch 50 Train-Accuracy 0.9775496578200634 Test-Accuracy 0.9753024193548387 Epoch 60 Loss 0.06835510271711621 Epoch 60 Train-Accuracy 0.9787180771156735 Test-Accuracy 0.9773185483870968 Epoch 70 Loss 0.06256444933163456 Epoch 70 Train-Accuracy 0.9808879986646637 Test-Accuracy 0.9793346774193549 Epoch 80 Loss 0.0573026369606475 Epoch 80 Train-Accuracy 0.9822233350025038 Test-Accuracy 0.9813508064516129 Epoch 90 Loss 0.05256537811821748 Epoch 90 Train-Accuracy 0.984727090635954 Test-Accuracy 0.9823588709677419 Epoch 100 Loss 0.04818521523931407 Epoch 100 Train-Accuracy 0.986312802537139 Test-Accuracy 0.9818548387096774 Epoch 110 Loss 0.044351783763895175 Epoch 110 Train-Accuracy 0.9879819729594391 Test-Accuracy 0.9823588709677419 Epoch 120 Loss 0.04081769252804259 Epoch 120 Train-Accuracy 0.9890669337339343 Test-Accuracy 0.983366935483871 Epoch 130 Loss 0.03762858377188597 Epoch 130 Train-Accuracy 0.9900684359873143 Test-Accuracy 0.984375 Epoch 140 Loss 0.03469955418461529 Epoch 140 Train-Accuracy 0.9909030211984644 Test-Accuracy 0.9863911290322581 Epoch 150 Loss 0.03202988575570701 Epoch 150 Train-Accuracy 0.9913203138040394 Test-Accuracy 0.9884072580645161 Epoch 160 Loss 0.029604722297956226 Epoch 160 Train-Accuracy 0.9924052745785344 Test-Accuracy 0.9889112903225806 Epoch 170 Loss 0.027353168578918223 Epoch 170 Train-Accuracy 0.9934067768319146 Test-Accuracy 0.9899193548387096 Epoch 180 Loss 0.025251161594384466 Epoch 180 Train-Accuracy 0.9936571523952595 Test-Accuracy 0.9899193548387096 Epoch 190 Loss 0.02331237291004236 Epoch 190 Train-Accuracy 0.9942413620430646 Test-Accuracy 0.9899193548387096 Epoch 200 Loss 0.021504820329789693 Epoch 200 Train-Accuracy 0.9947421131697546 Test-Accuracy 0.9909274193548387 Epoch 210 Loss 0.01985875265377323 Epoch 210 Train-Accuracy 0.9951594057753297 Test-Accuracy 0.9909274193548387 Epoch 220 Loss 0.018330257746630926 Epoch 220 Train-Accuracy 0.9962443665498247 Test-Accuracy 0.9914314516129032 Epoch 230 Loss 0.01691677500708628 Epoch 230 Train-Accuracy 0.9966616591553997 Test-Accuracy 0.9909274193548387 Epoch 240 Loss 0.015619790615715945 Epoch 240 Train-Accuracy 0.9969120347187448 Test-Accuracy 0.9909274193548387 . 0.0056728979927607665 . Analyzing and PLotting results . The model is already overfitting; the validation accuracy has dipped below the training accuracy, and they are diverging. Overfitting happens when the model is quite complex and is fitting to the noise in the training set rather than the signal. . Estimating Accuracies and Making predictions . #collapse nn_probs_train = nn_model(dataSet.X_train) print(nn_probs_train[:10]) nn_predictions_train = (nn_probs_train &gt;= 0.5)*1 nn_validateTrain = (nn_predictions_train == dataSet.y_train) nn_validateTrain_acc = np.mean(nn_validateTrain) &quot;Trani Accuracy is&quot;, nn_validateTrain_acc . . [[9.99999513e-01] [9.99979620e-01] [9.99999977e-01] [9.24930827e-04] [9.99999991e-01] [9.99440555e-01] [7.55556204e-05] [1.79033766e-04] [9.99856945e-01] [1.11085242e-03]] . (&#39;Trani Accuracy is&#39;, 0.9971624102820899) . #collapse nn_validateTest = (nn_model(dataSet.X_test) &gt;= 0.5)*1 == dataSet.y_test nn_validateTest_acc = np.mean(nn_validateTest) &quot;Test Accuracy is&quot;, nn_validateTest_acc . . (&#39;Test Accuracy is&#39;, 0.9919354838709677) . You can visually see some points stranded on the wrong side of the plot . Finding False Positives and False Negatives . #collapse from sklearn.metrics import confusion_matrix confMat = confusion_matrix(dataSet.y_train, nn_predictions_train) confMat . . array([[5840, 11], [ 23, 6108]], dtype=int64) . (&#39;FalseNegatives&#39;, 23) . (&#39;FalsePositives&#39;, 11) . Embedding Space . #collapse_show #Embedding space #layers = [Affine(&quot;FirstAffine&quot;, 784, 100), Relu(&quot;FirstRelu&quot;), Affine(&quot;SecondAffine&quot;, 100, 100), Relu(&quot;SecondRelu&quot;), Affine(&quot;Embedding&quot;, 100, 2), Affine(&quot;LastAffine&quot;, 2, 1), Sigmoid(&quot;LastSigmoid&quot;)] FirstAffine = nn_model.layers[0] FirstRelu = nn_model.layers[1] SecondAffine = nn_model.layers[2] SecondRelu = nn_model.layers[3] Embedding = nn_model.layers[4] embOutput = Embedding(SecondRelu(SecondAffine(FirstRelu(FirstAffine(dataSet.X_train))))) embOutput . . array([[-3.0569946 , -4.25882474], [-2.44295057, -3.0773888 ], [-3.53829114, -5.23932298], ..., [ 0.82998323, 2.59138162], [-1.7353985 , -1.95443277], [ 0.94543036, 2.72898146]]) . You can see a clear demarcation / seperation of points in the embedding space. We started off with (11982, 784) dimensional image matrix, here each row can now be taken as a vector of 784 dimensions, that is 784 distances along 784 different axes. Now at each layer these &quot;vectors&quot; gets &quot;transformed&quot; into different space and dimensions... So when we come to the embedding layer... these vectors gets &quot;embedded&quot; at different points in space in 2-D depending on where they were (or what they were) when they started out... so the 3&#39;s and 8&#39;s get seperated in the 2-D plot (ideally)... which can then be visualized. . (1000, 1000) . Visualizing the probabilities of points in 3-D space (following a sigmoidal shape), along with the predicted and actual values (The misclassified points can also be seen). The embedding space is also drawn for better understanding (The plot is visualized at multiple angles for better clarity). . Logistic Regression model . #collapse_show #Setting up model lr_layers = [Affine(&quot;FirstAffine&quot;, 784, 1), Sigmoid(&quot;LastSigmoid&quot;)] lr_model = Model(lr_layers) . . #collapse_show #Setting up training lr_learner = Learner(loss, lr_model, opt, config.num_epochs) lr_acc = ClfCallback(lr_learner, config.bs, dataSet) lr_learner.set_callbacks([lr_acc]) . . Training the Logistic Regression . lr_learner.train_loop(dl) . Epoch 0 Loss 0.5896279689253001 Epoch 0 Train-Accuracy 0.8871640794525121 Test-Accuracy 0.8901209677419355 Epoch 10 Loss 0.2563542603193134 Epoch 10 Train-Accuracy 0.9321482223335003 Test-Accuracy 0.9445564516129032 Epoch 20 Loss 0.20662835845943953 Epoch 20 Train-Accuracy 0.9404106159238859 Test-Accuracy 0.9495967741935484 Epoch 30 Loss 0.1833921049954954 Epoch 30 Train-Accuracy 0.9453346686696712 Test-Accuracy 0.954133064516129 Epoch 40 Loss 0.16936567792349005 Epoch 40 Train-Accuracy 0.9483391754298114 Test-Accuracy 0.9576612903225806 Epoch 50 Loss 0.1597662585192179 Epoch 50 Train-Accuracy 0.9504256384576866 Test-Accuracy 0.9606854838709677 Epoch 60 Loss 0.15270311466995024 Epoch 60 Train-Accuracy 0.9526790185277917 Test-Accuracy 0.9632056451612904 Epoch 70 Loss 0.14723968416390507 Epoch 70 Train-Accuracy 0.9543481889500918 Test-Accuracy 0.9637096774193549 Epoch 80 Loss 0.14285849460265812 Epoch 80 Train-Accuracy 0.9556000667668169 Test-Accuracy 0.9647177419354839 Epoch 90 Loss 0.139241618139777 Epoch 90 Train-Accuracy 0.957018861625772 Test-Accuracy 0.9647177419354839 Epoch 100 Loss 0.13620510878900882 Epoch 100 Train-Accuracy 0.957686529794692 Test-Accuracy 0.9647177419354839 Epoch 110 Loss 0.13359638399933277 Epoch 110 Train-Accuracy 0.9583541979636121 Test-Accuracy 0.9647177419354839 Epoch 120 Loss 0.13132294969023636 Epoch 120 Train-Accuracy 0.9589384076114171 Test-Accuracy 0.9657258064516129 Epoch 130 Loss 0.1293308404155781 Epoch 130 Train-Accuracy 0.9594391587381071 Test-Accuracy 0.9662298387096774 Epoch 140 Loss 0.12756084625599967 Epoch 140 Train-Accuracy 0.9598564513436821 Test-Accuracy 0.9667338709677419 Epoch 150 Loss 0.12597561077893552 Epoch 150 Train-Accuracy 0.9606075780337172 Test-Accuracy 0.967741935483871 Epoch 160 Loss 0.12454155489778115 Epoch 160 Train-Accuracy 0.9606910365548322 Test-Accuracy 0.9682459677419355 Epoch 170 Loss 0.12324329738223122 Epoch 170 Train-Accuracy 0.9609414121181773 Test-Accuracy 0.9682459677419355 Epoch 180 Loss 0.12205057565412171 Epoch 180 Train-Accuracy 0.9610248706392923 Test-Accuracy 0.96875 Epoch 190 Loss 0.12095844024342302 Epoch 190 Train-Accuracy 0.9612752462026373 Test-Accuracy 0.9682459677419355 Epoch 200 Loss 0.11995325223309848 Epoch 200 Train-Accuracy 0.9621098314137874 Test-Accuracy 0.96875 Epoch 210 Loss 0.11901914234016235 Epoch 210 Train-Accuracy 0.9625271240193624 Test-Accuracy 0.96875 Epoch 220 Loss 0.1181481735634853 Epoch 220 Train-Accuracy 0.9631113336671674 Test-Accuracy 0.967741935483871 Epoch 230 Loss 0.11733912141449204 Epoch 230 Train-Accuracy 0.9633617092305125 Test-Accuracy 0.9672379032258065 Epoch 240 Loss 0.11657877915103394 Epoch 240 Train-Accuracy 0.9636120847938574 Test-Accuracy 0.967741935483871 . 0.1305527859517604 . (&#39;Train Accuracy&#39;, 0.9640293773994325) . (&#39;Test Accuracy&#39;, 0.9672379032258065) . Comparing Multi Layer Perceptron with Logistic Regression . The Multi Layer Perceptron has greater accuracy and hence can be deemed to have a better performance than the Logistic Regression model. . Result . Taking a random image and classifying it as 3 or 8 . We can say with 99.19% certainity that: . The above Image is a &#39;3&#39; (Three) - - - - - - - - - - - - - - - - - - - - . The above Image is a &#39;3&#39; (Three) - - - - - - - - - - - - - - - - - - - - . The above Image is a &#39;8&#39; (Eight) - - - - - - - - - - - - - - - - - - - - . The above Image is a &#39;3&#39; (Three) - - - - - - - - - - - - - - - - - - - - . The above Image is a &#39;3&#39; (Three) - - - - - - - - - - - - - - - - - - - - . The above Image is a &#39;3&#39; (Three) - - - - - - - - - - - - - - - - - - - - . The above Image is a &#39;3&#39; (Three) - - - - - - - - - - - - - - - - - - - - . The above Image is a &#39;3&#39; (Three) - - - - - - - - - - - - - - - - - - - - . The above Image is a &#39;8&#39; (Eight) - - - - - - - - - - - - - - - - - - - - . The above Image is a &#39;3&#39; (Three) - - - - - - - - - - - - - - - - - - - - . Categorical classification of all numbers . Using softmax activation and classification cross entropy . Yet to be implemented . from kudzu.layer import Softmax from sklearn.preprocessing import OneHotEncoder . #collapse_show train_images = mnist.train_images() train_labels = mnist.train_labels() test_images = mnist.test_images() test_labels = mnist.test_labels() . . #collapse_show one_hot_encoder = OneHotEncoder(sparse=False) y_train = one_hot_encoder.fit_transform(train_labels.reshape(-1,1)) print(y_train, y_train[0], train_labels[0]) y_test = one_hot_encoder.fit_transform(test_labels.reshape(-1,1)) X_train = train_images/255. X_test = test_images/255. . . [[0. 0. 0. ... 0. 0. 0.] [1. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 1. 0.]] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] 5 . #collapse_show #Setting up model cc_layers = [Affine(&quot;FirstAffine&quot;, 784, 64), Relu(&quot;FirstRelu&quot;), Affine(&quot;SecondAffine&quot;, 64, 64), Relu(&quot;SecondRelu&quot;), Affine(&quot;LastAffine&quot;, 64, 10), Softmax(&quot;LastSoftmax&quot;)] cc_model = Model(cc_layers) . .",
            "url": "https://prasannadk.github.io/UnivAI-FastBlogs/fastpages/jupyter/2020/08/06/Kudzu-ANN.html",
            "relUrl": "/fastpages/jupyter/2020/08/06/Kudzu-ANN.html",
            "date": " • Aug 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "BYOB - COVID-19 India Matplotlib Overview (as on 28th July 2020)",
            "content": "India . Last update: 28th July, 2020 . Confirmed cases: . 1514800 (+49001) . Confirmed deaths: . 34121 (+770) . states Cases Deaths PCases PDeaths Cases (+) Deaths (+) Fatality Rate Maharashtra 391440 14164 383723 13882 7717 282 3.62 Tamil Nadu 227688 3659 220716 3571 6972 88 1.61 Delhi 132275 3881 131219 3853 1056 28 2.93 Andhra Pradesh 110297 1148 102349 1090 7948 58 1.04 Karnataka 107001 2064 101465 1962 5536 102 1.93 Uttar Pradesh 73951 1497 70493 1456 3458 41 2.02 West Bengal 62964 1449 60830 1411 2134 38 2.30 Gujarat 57982 2372 56874 2348 1108 24 4.09 Telangana 57142 480 55532 471 1610 9 0.84 Bihar 43591 269 41111 255 2480 14 0.62 Rajasthan 38636 644 37564 633 1072 11 1.67 Assam 34846 92 33475 90 1371 2 0.26 Haryana 32876 406 32127 397 749 9 1.23 Madhya Pradesh 29217 831 28589 821 628 10 2.84 Orissa 28107 189 26892 181 1215 8 0.67 Kerala 20895 68 19728 64 1167 4 0.33 Jammu and Kashmir 18879 333 18390 321 489 12 1.76 Punjab 14378 336 13769 318 609 18 2.34 Jharkhand 9563 94 8803 90 760 4 0.98 Goa 5287 36 5119 36 168 0 0.68 Tripura 4287 21 4066 17 221 4 0.49 Pondicherry 3013 47 2874 43 139 4 1.56 Himachal Pradesh 2330 13 2270 13 60 0 0.56 Manipur 2317 0 2286 0 31 0 0.00 Nagaland 1460 4 1385 5 75 0 0.27 Arunachal Pradesh 1330 3 1239 3 91 0 0.23 Chandigarh 934 14 910 14 24 0 1.50 Meghalaya 779 5 738 5 41 0 0.64 Sikkim 592 1 568 1 24 0 0.17 Mizoram 384 0 361 0 23 0 0.00 Andaman and Nicobar Islands 359 1 334 1 25 0 0.28 Daman and Diu 0 0 0 0 0 0 NaN Lakshadweep 0 0 0 0 0 0 NaN .",
            "url": "https://prasannadk.github.io/UnivAI-FastBlogs/fastpages/jupyter/2020/08/06/COVID-19-Dashboard.html",
            "relUrl": "/fastpages/jupyter/2020/08/06/COVID-19-Dashboard.html",
            "date": " • Aug 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://prasannadk.github.io/UnivAI-FastBlogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://prasannadk.github.io/UnivAI-FastBlogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}